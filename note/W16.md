# 20210610
* 人工智慧>機器學習>深度學習
## [蒙地卡羅方法](https://zh.wikipedia.org/wiki/%E8%92%99%E5%9C%B0%E5%8D%A1%E7%BE%85%E6%96%B9%E6%B3%95)
* 基本概念
    1. 所求解的問題本身具有內在的隨機性，藉助電腦的運算能力可以直接類比這種隨機的過程
    2. 所求解問題可以轉化為某種隨機分布的特徵數，比如隨機事件出現的機率，或者隨機變數的期望值\
![PICTURE](https://github.com/victor0520/ai109b/blob/main/note/bitmap/monte_carlo.png)
## [馬可夫鏈](https://programmermedia.org/root/%E9%99%B3%E9%8D%BE%E8%AA%A0/%E8%AA%B2%E7%A8%8B/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7/_doc/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/B2-%E9%A6%AC%E5%8F%AF%E5%A4%AB%E9%8F%88.md)
* 「馬可夫鏈」是一種具有狀態的隨機過程，有點像是「有限狀態機」
* 為狀態空間中經過從一個狀態到另一個狀態的轉換的隨機過程
* 該過程要求具備「無記憶」的性質：下一狀態的機率分布只能由當前狀態決定，在時間序列中它前面的事件均與之無關。這種特定類型的「無記憶性」稱作馬可夫性質
* 下圖顯示了一個只有兩個狀態的「馬可夫隨機系統」\
    ![PICTURE](https://github.com/victor0520/ai109b/blob/main/note/bitmap/markov2state.jpg)

## [吉布斯採樣](https://zh.wikipedia.org/wiki/%E5%90%89%E5%B8%83%E6%96%AF%E9%87%87%E6%A0%B7)
* 吉布斯採樣是統計學中用於馬爾科夫蒙特卡洛（MCMC）的一種算法
* 用於在難以直接採樣時從某一多變量概率分布中近似抽取樣本序列
* 吉布斯採樣適用於條件分布比邊緣分布更容易採樣的多變量分布\
![PICTURE](https://github.com/victor0520/ai109b/blob/main/note/bitmap/gibbs.png)
## [隱藏式馬可夫模型](https://zh.wikipedia.org/wiki/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B)
* 隱藏式馬可夫模型或，是統計模型，它用來描述一個含有隱含未知參數的馬可夫過程
* 其難點是從可觀察的參數中確定該過程的隱含參數。然後利用這些參數來作進一步的分析，例如圖型識別\
![PICTURE](https://github.com/victor0520/ai109b/blob/main/note/bitmap/Hidden_Markov.png)

## [維特比演算法](https://zh.wikipedia.org/wiki/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95)
* 維特比演算法是一種動態規劃演算法
* 用於尋找最有可能產生觀測事件序列的維特比路徑——隱含狀態序列，特別是在馬可夫資訊源上下文和隱藏式馬可夫模型中

## [最大期望算法](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E6%9C%9F%E6%9C%9B%E7%AE%97%E6%B3%95)
* 最大期望演算法在統計中被用於尋找，依賴於不可觀察的隱性變量的概率模型中，參數的最大似然估計
* 在統計計算中，最大期望（EM）算法是在概率模型中尋找參數最大似然估計或者最大後驗估計的算法，其中概率模型依賴於無法觀測的隱變量
* 最大期望算法經常用在機器學習和計算機視覺的數據聚類領域
* [EM 演算法實作](https://programmermedia.org/root/%E9%99%B3%E9%8D%BE%E8%AA%A0/%E8%AA%B2%E7%A8%8B/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7/06-learn/05-em/em.md)
* 範例:\
    ![PICTURE](https://github.com/victor0520/ai109b/blob/main/note/bitmap/EM-1.png)
    *  H 代表正面 (Head) ，而 T 代表反面 (Tail)
* 下圖顯示了 EM 演算法在這個範例上的運作過程\
    ![PICTURE](https://github.com/victor0520/ai109b/blob/main/note/bitmap/EM-2.png)

## [K-近鄰演算法](https://ithelp.ithome.com.tw/articles/10197110)
* k是一個用戶定義的常數。一個沒有類別標籤的向量（查詢或測試點）將被歸類為最接近該點的k個樣本點中最頻繁使用的一類。\
![PICTURE](https://github.com/victor0520/ai109b/blob/main/note/bitmap/KNN.png)

## [sklearn](https://zh.wikipedia.org/wiki/Scikit-learn)
* 用於Python程式語言的自由軟體機器學習庫
* 特徵是具有各種分類、回歸和聚類算法，包括支持向量機、隨機森林、梯度提升、k-平均聚類和DBSCAN
* 被設計協同於Python數值和科學庫NumPy和SciPy
* 安裝:`pip install sklearn`

## knn.py
```
$ python knn.py
預測答案： [1 1 1 2 0 1 2 0 1 1 1 0 2 1 1 2 2 2 0 2 1 0 0 0 0 
0 2 1 1 1]
正確答案： [1 1 1 1 0 1 2 0 1 1 1 0 2 1 1 2 2 2 0 2 1 0 0 0 0 
0 2 1 1 1]
```
## [支援向量機(Support Vector Machine)介紹](https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-4%E8%AC%9B-%E6%94%AF%E6%8F%B4%E5%90%91%E9%87%8F%E6%A9%9F-support-vector-machine-%E4%BB%8B%E7%B4%B9-9c6c6925856b)

## [決策樹(Decision Tree)以及隨機森林(Random Forest)介紹](https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)
## 參考資料
* [馬可夫鏈](https://zh.wikipedia.org/wiki/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE)
* [MCMC(四)Gibbs采样](https://www.cnblogs.com/pinard/p/6645766.html)
* [維特比演算法（Viterbi algorithm）](https://programmermedia.org/root/%E9%99%B3%E9%8D%BE%E8%AA%A0/%E8%AA%B2%E7%A8%8B/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7/_doc/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/B3a-%E7%B6%AD%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95.md)
* [K-近鄰演算法](https://zh.wikipedia.org/wiki/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95)
