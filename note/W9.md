# 20210422
## 神經網路介紹
* 人工神經網路，簡稱神經網路或類神經網路，在機器學習和認知科學領域，是一種模仿生物神經網路的結構和功能的數學模型或計算模型，用於對函式進行估計或近似
* 大多數情況下人工神經網路能在外界資訊的基礎上改變內部結構，是一種自適應系統，通俗地講就是具備學習功能
* 公式
![PICTURE](https://github.com/victor0520/ai109b/tree/main/note/bitmap/Neural_formula.png)
* 示意圖
![PICTURE](https://github.com/victor0520/ai109b/tree/main/note/bitmap/Neural.png)

## 單變數微分
### diff
* code
```
def f(x):
    # return x*x
    return x**3

dx = 0.001 /求逼近

def diff(f, x):
    df = f(x+dx)-f(x)
    return df/dx

print('diff(f,2)=', diff(f, 2))
```
* 執行結果
```
$ python diff.py
diff(f,2)= 12.006000999997823
```
### e (自然常數、自然底數)
* e的微分是自己，積分也是自己
* 執行結果會趨近於e
* code
```
def e(n):
	return (1+1.0/n)**n

for i in range(100):
	n = (i+1)*100.0
	print('n=', n, 'e(n)=', e(n))
```
* 執行結果
```
$ python e.py
n= 100.0 e(n)= 2.7048138294215285
n= 200.0 e(n)= 2.711517122929317
n= 300.0 e(n)= 2.7137651579427837
n= 400.0 e(n)= 2.7148917443812293
n= 500.0 e(n)= 2.715568520651728
n= 600.0 e(n)= 2.7160200488806514
n= 700.0 e(n)= 2.7163427377295566
n= 800.0 e(n)= 2.716584846682471
n= 900.0 e(n)= 2.716773208380411
n= 1000.0 e(n)= 2.7169239322355936
n= 1100.0 e(n)= 2.717047274569425
.
.
.
n= 8600.0 e(n)= 2.71812380566317
n= 8700.0 e(n)= 2.7181256218249055
n= 8800.0 e(n)= 2.718127396711982
n= 8900.0 e(n)= 2.718129131725891
n= 9000.0 e(n)= 2.7181308281830128
n= 9100.0 e(n)= 2.718132487359168
n= 9200.0 e(n)= 2.718134110467929
n= 9300.0 e(n)= 2.718135698675662
n= 9400.0 e(n)= 2.718137253097062
n= 9500.0 e(n)= 2.7181387748001744
n= 9600.0 e(n)= 2.718140264795318
n= 9700.0 e(n)= 2.718141724076723
n= 9800.0 e(n)= 2.718143153583405
n= 9900.0 e(n)= 2.718144554210053
n= 10000.0 e(n)= 2.7181459268249255
```

## [梯度下降](http://programmermedia.org/root/%E9%99%B3%E9%8D%BE%E8%AA%A0/%E8%AA%B2%E7%A8%8B/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7/07-neural/02-gradient/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.md)
* 深度學習 (Deep Learning) 是人工智慧領域當紅的技術，說穿了其實就是原本的《神經網路》(Neural Network) ，不過由於加上了一些新的模型 (像是捲積神經網路 CNN, 循環神經網路 RNN 與生成對抗網路 GAN)，還有在神經網路的層數上加深很多，從以往的 3-4 層，提升到了十幾層，甚至上百層，於是我們給這些新一代的《神經網路》技術一個統稱，那就是《深度學習》
![PICTURE](http://programmermedia.org/root/%E9%99%B3%E9%8D%BE%E8%AA%A0/%E8%AA%B2%E7%A8%8B/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7/07-neural/img/Gradient.jpg)
* 其實梯度就是斜率最大的那個方向，所以梯度下降法，其實就是朝著斜率最大的方向走。

* 如果我們朝著《斜率最大》方向的《正梯度》走，那麼就會愈走愈高，但是如果朝著《逆梯度》方向走，那麼就會愈走愈低。

* 而梯度下降法，就是朝著《逆梯度》的方向走，於是就可以不斷下降，直到到達梯度為 0 的點 (斜率最大的方向仍然是斜率為零)，此時就已經到了一個《谷底》，也就是區域的最低點了